DATA:
  data_root:  '/data/mdw/datasets/VOC/VOC2012/'
  train_list:  '/data/mdw/few_shot_segmentation/data/lists/pascal/voc_sbd_merge_noduplicate.txt'
  val_list: '/data/mdw/few_shot_segmentation/data/lists/pascal/val.txt'
  classes: 2
#DATA:
#  data_root: 'E:\code\few-shot_segmentation\My_Model\datasets\VOC\VOC2012'
#  train_list: 'E:\code\few-shot_segmentation\My_Model\PFENet\lists\pascal\voc_sbd_merge.txt'
#  val_list: 'E:\code\few-shot_segmentation\My_Model\PFENet\lists\pascal\val.txt's
#  classes: 2

TRAIN:
  arch: asgnet #asgnet  # _memory_bank_v3  #asgnet_memory_bank_v3
  layers: 50
  sync_bn: False
  train_h: 473
  train_w: 473
  val_size: 473
  scale_min: 0.9  # minimum random scale
  scale_max: 1.1 # maximum random scale
  rotate_min: -10  # minimum random rotate
  rotate_max: 10  # maximum random rotate
  zoom_factor: 8  # zoom factor for final prediction during training, be in [1, 2, 4, 8]
  ignore_label: 255
  padding_label: 255
  aux_weight: 1.0
  train_gpu: [0]
  workers: 1  # data loader workers
  batch_size: 8  # batch size for training
  batch_size_val: 1
  base_lr: 0.0025
  epochs: 200
  start_epoch: 0
  power: 0.9 # 0 means no decay
  momentum: 0.9
  weight_decay: 0.0001
  manual_seed: 321
  print_freq: 5
  save_freq: 20
  save_path: exp/asgnet_one_prototype/pascal/split0_resnet50/model
  weight: exp/asgnet_one_prototype/pascal/split0_resnet50/model/train_epoch_144_0.5704117743949944.pth # exp/asgnet_memory_bank_v4/pascal/split0_resnet50/model/train_epoch_28_0.5750041986747068.pth # exp/asgnet_memory_bank/pascal/split0_resnet50/model/train_epoch_88_0.5757419312299488.pth # exp/asgnet_memory_bank/pascal/split1_resnet50/model/train_epoch_104_0.6866028573329549.pth 
  resume:
  evaluate: True
  split: 0
  shot: 1
  max_sp: 1
  train_iter: 10
  eval_iter: 5
  pyramid: True
  ppm_scales: [60, 30, 15, 8]
  fix_random_seed_val: True
  warmup: False
  use_coco: False
  use_split_coco: False
  resized_val: True
  ori_resize: True  # use original label for evaluation

## deprecated multi-processing training
Distributed:
  dist_url: tcp://127.0.0.1:6789
  dist_backend: 'nccl'
  multiprocessing_distributed: False
  world_size: 1
  rank: 0
  use_apex: False
  opt_level: 'O0'
  keep_batchnorm_fp32:
  loss_scale:

